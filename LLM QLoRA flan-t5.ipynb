{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-27T01:54:10.145917Z",
     "iopub.status.busy": "2023-09-27T01:54:10.145218Z",
     "iopub.status.idle": "2023-09-27T01:54:13.942427Z",
     "shell.execute_reply": "2023-09-27T01:54:13.938607Z",
     "shell.execute_reply.started": "2023-09-27T01:54:10.145885Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dataiku/data/code-envs/python/test_GPU/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "print(torch.cuda.is_available())\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n",
    "os.environ['WANDB_DISABLED'] = 'true'\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import transformers\n",
    "import accelerate\n",
    "import tensorboard\n",
    "import bitsandbytes as bnb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Quantization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BitsAndBytesConfig\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Model lnitialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading model.safetensors:  30%|███       | 912M/3.00G [08:13<18:50, 1.85MB/s]\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer\n",
    "\n",
    "model_id = \"google/flan-t5-base\"\n",
    "\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(\n",
    "        model_id, \n",
    "        quantization_config = bnb_config, \n",
    "        torch_dtype = torch.float16,\n",
    "        device_map = {\"\":0}\n",
    ")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-09-27T01:54:13.951013Z",
     "iopub.status.idle": "2023-09-27T01:54:13.953106Z",
     "shell.execute_reply": "2023-09-27T01:54:13.952891Z",
     "shell.execute_reply.started": "2023-09-27T01:54:13.952865Z"
    }
   },
   "outputs": [],
   "source": [
    "for param in model.parameters():\n",
    "    param.requires_grad = False  # freeze the model - train adapters later\n",
    "    if param.ndim == 1:\n",
    "        # cast the small parameters (e.g. layernorm) to fp32 for stability\n",
    "        param.data = param.data.to(torch.float32)\n",
    "\n",
    "model.gradient_checkpointing_enable()  # reduce number of stored activations\n",
    "model.enable_input_require_grads()\n",
    "\n",
    "class CastOutputToFloat(nn.Sequential):\n",
    "    def forward(self, x):\n",
    "        return super().forward(x).to(torch.float32)\n",
    "model.lm_head = CastOutputToFloat(model.lm_head)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Observe model output before fine-tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display, Markdown\n",
    "\n",
    "def make_inference(model, context, question, max_new_tokens=200):\n",
    "    batch = tokenizer(f\"#### CONTEXT\\n{context}\\n\\n#### QUESTION\\n{question}\\n\\n#### ANSWER\\n\", return_tensors='pt', return_token_type_ids=False).to('cuda')\n",
    "\n",
    "    with torch.cuda.amp.autocast():\n",
    "        output_tokens = model.generate(**batch, max_new_tokens=max_new_tokens)\n",
    "\n",
    "    display(Markdown((tokenizer.decode(output_tokens[0], skip_special_tokens=True))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Cheese"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "context = \"Cheese is the best food.\"\n",
    "question = \"What is the best food?\"\n",
    "\n",
    "make_inference(model, context, question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-09-27T01:54:13.998399Z",
     "iopub.status.idle": "2023-09-27T01:54:13.999126Z",
     "shell.execute_reply": "2023-09-27T01:54:13.998934Z",
     "shell.execute_reply.started": "2023-09-27T01:54:13.998914Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "The Moon is approximately 1.3 billion light years away."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "context = \"Cheese is the best food.\"\n",
    "question = \"How far away is the Moon from the Earth?\"\n",
    "\n",
    "make_inference(model, context, question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "30 times Earth's diameter"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "context = \"The Moon orbits Earth at an average distance of 384,400 km (238,900 miles), or about 30 times Earth's diameter. Its gravitational influence is the main driver of Earth's tides and very slowly lengthens Earth's day. The Moon's orbit around Earth has a sidereal period of 27.3 days. During each synodic period of 29.5 days, the amount of visible surface illuminated by the Sun varies from none up to 100%, resulting in lunar phases that form the basis for the months of a lunar calendar. The Moon is tidally locked to Earth, which means that the length of a full rotation of the Moon on its own axis causes its same side (the near side) to always face Earth, and the somewhat longer lunar day is the same as the synodic period. However, 59% of the total lunar surface can be seen from Earth through cyclical shifts in perspective known as libration.\"\n",
    "question = \"At what distance does the Moon orbit the Earth?\"\n",
    "\n",
    "make_inference(model, context, question)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-09-27T01:54:13.956671Z",
     "iopub.status.idle": "2023-09-27T01:54:13.958785Z",
     "shell.execute_reply": "2023-09-27T01:54:13.958566Z",
     "shell.execute_reply.started": "2023-09-27T01:54:13.958525Z"
    }
   },
   "outputs": [],
   "source": [
    "def print_trainable_parameters(model):\n",
    "    \"\"\"\n",
    "    Prints the number of trainable parameters in the model.\n",
    "    \"\"\"\n",
    "    trainable_params = 0\n",
    "    all_param = 0\n",
    "    for _, param in model.named_parameters():\n",
    "        all_param += param.numel()\n",
    "        if param.requires_grad:\n",
    "            trainable_params += param.numel()\n",
    "    print(\n",
    "        f\"trainable params: {trainable_params} || all params: {all_param} || trainable%: {100 * trainable_params / all_param}\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_prompt(context, question, answer):\n",
    "    if len(answer[\"text\"]) < 1:\n",
    "        answer = \"Cannot Find Answer\"\n",
    "    else:\n",
    "        answer = answer[\"text\"][0]\n",
    "    prompt_template = f\"### CONTEXT\\n{context}\\n\\n### QUESTION\\n{question}\\n\\n### ANSWER\\n{answer}</s>\"\n",
    "    return prompt_template"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " #### 5. Load preprocessed dataset from disk (skip to step 7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Load from disk\n",
    "\n",
    "from datasets import load_dataset, Dataset, load_from_disk\n",
    "\n",
    "dataset = load_from_disk('/home/dataiku/data/saved_datasets/squad/raw/train')\n",
    "tokenized_dataset = load_from_disk('/home/dataiku/data/saved_datasets/squad/tokenized/train')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6a. Load raw dataset from HuggingFace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "43498 rows removed.\n",
      "Train dataset size: 13023\n",
      "Test dataset size: 1737\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Saving the dataset (0/1 shards):   0%|          | 0/13023 [00:00<?, ? examples/s]\u001b[A\n",
      "Saving the dataset (0/1 shards):  15%|█▌        | 2000/13023 [00:00<00:00, 19896.18 examples/s]\u001b[A\n",
      "Saving the dataset (0/1 shards):  54%|█████▍    | 7000/13023 [00:00<00:00, 31883.86 examples/s]\u001b[A\n",
      "Saving the dataset (0/1 shards): 100%|█████████▉| 13000/13023 [00:00<00:00, 37680.72 examples/s]\u001b[A\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 13023/13023 [00:00<00:00, 32627.31 examples/s]\u001b[A\n",
      "\n",
      "Saving the dataset (0/1 shards):   0%|          | 0/1737 [00:00<?, ? examples/s]\u001b[A\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 1737/1737 [00:00<00:00, 20940.78 examples/s]\u001b[A\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset, Dataset\n",
    "\n",
    "dataset = load_dataset(\"squad_v2\")\n",
    "dataset = pd.DataFrame(dataset['train'])\n",
    "\n",
    "# remove rows with empty answers\n",
    "exclude = []\n",
    "for i in range(len(dataset)):\n",
    "    if not dataset.iloc[i]['answers']['text']:\n",
    "        exclude.append(i)\n",
    "dataset = dataset.drop(exclude)\n",
    "print(f'{len(exclude)} rows removed.')\n",
    "\n",
    "# accept only the first answer in every line of data\n",
    "answer = []\n",
    "for i in range(len(dataset)):\n",
    "    answer.append(dataset.iloc[i]['answers']['text'][0])\n",
    "dataset['answer'] = answer\n",
    "\n",
    "dataset = Dataset.from_pandas(dataset)\n",
    "dataset = dataset.train_test_split(train_size=0.15, test_size=0.02) # smaller dataset\n",
    "\n",
    "dataset[\"validation\"] = dataset[\"test\"]\n",
    "del dataset[\"test\"]\n",
    "\n",
    "print(f\"Train dataset size: {len(dataset['train'])}\")\n",
    "print(f\"Test dataset size: {len(dataset['validation'])}\")\n",
    "\n",
    "\n",
    "## Save dataset to disk for easy loading later\n",
    "\n",
    "dataset_path = '/home/dataiku/data/saved_datasets/squad'\n",
    "dataset.save_to_disk(f'{dataset_path}/raw')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['input_ids', 'attention_mask', 'labels'],\n",
       "    num_rows: 13023\n",
       "})"
      ]
     },
     "execution_count": 230,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## To load dataset from local directory\n",
    "\n",
    "load_from_disk('/home/dataiku/data/saved_datasets/squad/raw/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6b. Preprocess training dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Map:   0%|          | 0/14760 [00:00<?, ? examples/s]\u001b[A\n",
      "Map:   7%|▋         | 1000/14760 [00:00<00:03, 3670.79 examples/s]\u001b[A\n",
      "Map:  14%|█▎        | 2000/14760 [00:00<00:03, 3899.45 examples/s]\u001b[A\n",
      "Map:  20%|██        | 3000/14760 [00:00<00:02, 4069.40 examples/s]\u001b[A\n",
      "Map:  27%|██▋       | 4000/14760 [00:00<00:02, 4224.46 examples/s]\u001b[A\n",
      "Map:  34%|███▍      | 5000/14760 [00:01<00:02, 4216.91 examples/s]\u001b[A\n",
      "Map:  41%|████      | 6000/14760 [00:01<00:02, 3807.28 examples/s]\u001b[A\n",
      "Map:  47%|████▋     | 7000/14760 [00:01<00:02, 3835.22 examples/s]\u001b[A\n",
      "Map:  54%|█████▍    | 8000/14760 [00:02<00:01, 3896.34 examples/s]\u001b[A\n",
      "Map:  61%|██████    | 9000/14760 [00:02<00:01, 3899.55 examples/s]\u001b[A\n",
      "Map:  68%|██████▊   | 10000/14760 [00:02<00:01, 3824.29 examples/s]\u001b[A\n",
      "Map:  75%|███████▍  | 11000/14760 [00:02<00:00, 3814.15 examples/s]\u001b[A\n",
      "Map:  81%|████████▏ | 12000/14760 [00:03<00:00, 3879.74 examples/s]\u001b[A\n",
      "Map:  88%|████████▊ | 13000/14760 [00:03<00:00, 3874.62 examples/s]\u001b[A\n",
      "Map:  95%|█████████▍| 14000/14760 [00:03<00:00, 3866.30 examples/s]\u001b[A\n",
      "Map: 100%|██████████| 14760/14760 [00:03<00:00, 3886.05 examples/s]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max source length: 243\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Map:   0%|          | 0/14760 [00:00<?, ? examples/s]\u001b[A\n",
      "Map:  20%|██        | 3000/14760 [00:00<00:00, 20649.42 examples/s]\u001b[A\n",
      "Map:  41%|████      | 6000/14760 [00:00<00:00, 21161.22 examples/s]\u001b[A\n",
      "Map:  61%|██████    | 9000/14760 [00:00<00:00, 22118.97 examples/s]\u001b[A\n",
      "Map:  81%|████████▏ | 12000/14760 [00:01<00:00, 9087.57 examples/s]\u001b[A\n",
      "Map: 100%|██████████| 14760/14760 [00:01<00:00, 12504.39 examples/s]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max target length: 11\n"
     ]
    }
   ],
   "source": [
    "from datasets import concatenate_datasets\n",
    "\n",
    "## Determine maximum total input sequence length after tokenization => \n",
    "## Sequences beyond this will be truncated, sequences shorter will be padded\n",
    "\n",
    "tokenized_inputs = concatenate_datasets([dataset[\"train\"], dataset[\"validation\"]]).map(lambda x: tokenizer(x[\"context\"], truncation=True), batched=True, remove_columns=['id', 'title', 'context', 'question', 'answers', 'answer', '__index_level_0__'])\n",
    "input_lengths = [len(x) for x in tokenized_inputs[\"input_ids\"]]\n",
    "max_source_length = int(np.percentile(input_lengths, 85))    # 85% of max length for better utilization\n",
    "print(f\"Max source length: {max_source_length}\")\n",
    "\n",
    "\n",
    "## Determine maximum total sequence length for target text after tokenization =>  \n",
    "## Sequences beyond this will be truncated, sequences shorter will be padded\n",
    "tokenized_targets = concatenate_datasets([dataset[\"train\"], dataset[\"validation\"]]).map(lambda x: tokenizer(x[\"answer\"], truncation=True), batched=True, remove_columns=['id', 'title', 'context', 'question', 'answers', 'answer', '__index_level_0__'])\n",
    "target_lengths = [len(x) for x in tokenized_targets[\"input_ids\"]]\n",
    "max_target_length = int(np.percentile(target_lengths, 90))    # 90% of max length for better utilization\n",
    "print(f\"Max target length: {max_target_length}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Running tokenizer on dataset:   0%|          | 0/13023 [00:00<?, ? examples/s]\u001b[A\n",
      "Running tokenizer on dataset:   8%|▊         | 1000/13023 [00:00<00:03, 3401.40 examples/s]\u001b[A\n",
      "Running tokenizer on dataset:  15%|█▌        | 2000/13023 [00:00<00:03, 3410.19 examples/s]\u001b[A\n",
      "Running tokenizer on dataset:  23%|██▎       | 3000/13023 [00:00<00:03, 3085.31 examples/s]\u001b[A\n",
      "Running tokenizer on dataset:  31%|███       | 4000/13023 [00:01<00:02, 3129.54 examples/s]\u001b[A\n",
      "Running tokenizer on dataset:  38%|███▊      | 5000/13023 [00:01<00:02, 3187.97 examples/s]\u001b[A\n",
      "Running tokenizer on dataset:  46%|████▌     | 6000/13023 [00:01<00:02, 3242.43 examples/s]\u001b[A\n",
      "Running tokenizer on dataset:  54%|█████▍    | 7000/13023 [00:02<00:01, 3286.57 examples/s]\u001b[A\n",
      "Running tokenizer on dataset:  61%|██████▏   | 8000/13023 [00:02<00:01, 3260.45 examples/s]\u001b[A\n",
      "Running tokenizer on dataset:  69%|██████▉   | 9000/13023 [00:02<00:01, 3199.85 examples/s]\u001b[A\n",
      "Running tokenizer on dataset:  77%|███████▋  | 10000/13023 [00:03<00:00, 3129.30 examples/s]\u001b[A\n",
      "Running tokenizer on dataset:  84%|████████▍ | 11000/13023 [00:03<00:00, 3121.56 examples/s]\u001b[A\n",
      "Running tokenizer on dataset:  92%|█████████▏| 12000/13023 [00:03<00:00, 2975.79 examples/s]\u001b[A\n",
      "Running tokenizer on dataset: 100%|██████████| 13023/13023 [00:04<00:00, 3063.69 examples/s]\u001b[A\n",
      "\n",
      "Running tokenizer on dataset:   0%|          | 0/1737 [00:00<?, ? examples/s]\u001b[A\n",
      "Running tokenizer on dataset:  58%|█████▊    | 1000/1737 [00:00<00:00, 2924.03 examples/s]\u001b[A\n",
      "Running tokenizer on dataset: 100%|██████████| 1737/1737 [00:00<00:00, 2605.80 examples/s]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keys of tokenized dataset: ['input_ids', 'attention_mask', 'labels']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Saving the dataset (0/1 shards):   0%|          | 0/13023 [00:00<?, ? examples/s]\u001b[A\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 13023/13023 [00:00<00:00, 180625.52 examples/s]\u001b[A\n",
      "\n",
      "Saving the dataset (0/1 shards):   0%|          | 0/1737 [00:00<?, ? examples/s]\u001b[A\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 1737/1737 [00:00<00:00, 76934.92 examples/s]\u001b[A\n"
     ]
    }
   ],
   "source": [
    "def preprocess_function(sample, padding=\"max_length\"):\n",
    "    # add prefix to the input for t5\n",
    "    inputs = [f'context: {i} question: {j}' for i, j in zip(sample[\"context\"], sample[\"question\"])]\n",
    "\n",
    "    # tokenize inputs\n",
    "    model_inputs = tokenizer(inputs, max_length=max_source_length, padding=padding, truncation=True)\n",
    "\n",
    "    # Tokenize targets with the `text_target` keyword argument\n",
    "    labels = tokenizer(text_target=sample[\"answer\"], max_length=max_target_length, padding=padding, truncation=True)\n",
    "\n",
    "    # If we are padding here, replace all tokenizer.pad_token_id in the labels by -100 when we want to ignore\n",
    "    # padding in the loss.\n",
    "    if padding == \"max_length\":\n",
    "        labels[\"input_ids\"] = [\n",
    "            [(l if l != tokenizer.pad_token_id else -100) for l in label] for label in labels[\"input_ids\"]\n",
    "        ]\n",
    "\n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    return model_inputs\n",
    "\n",
    "tokenized_dataset = dataset.map(preprocess_function, \n",
    "                                batched=True, \n",
    "                                remove_columns=['id', 'title', 'context', 'question', 'answers', 'answer', '__index_level_0__'], \n",
    "                                desc=\"Running tokenizer on dataset\")\n",
    "print(f\"Keys of tokenized dataset: {list(tokenized_dataset['train'].features)}\")\n",
    "\n",
    "      \n",
    "## Save tokenized_dataset to disk for later easy loading\n",
    "\n",
    "dataset_path = '/home/dataiku/data/saved_datasets/squad/tokenized'\n",
    "tokenized_dataset['train'].save_to_disk(f'{dataset_path}/train')\n",
    "tokenized_dataset['validation'].save_to_disk(f'{dataset_path}/test')   # used for evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 7. Fine-Tune T5 with LoRA and bnb int-8\n",
    "\n",
    "In addition to the LoRA technique, we will use [bitsanbytes LLM.int8()](https://huggingface.co/blog/hf-bitsandbytes-integration) to quantize out frozen LLM to int8. This allows us to reduce the needed memory for FLAN-T5 XXL ~4x.  \n",
    "\n",
    "The first step of our training is to load the model. We are going to use [philschmid/flan-t5-xxl-sharded-fp16](https://huggingface.co/philschmid/flan-t5-xxl-sharded-fp16), which is a sharded version of [google/flan-t5-xxl](https://huggingface.co/google/flan-t5-xxl). The sharding will help us to not run off of memory when loading the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer\n",
    "\n",
    "model_id = \"google/flan-t5-base\"\n",
    "\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(\n",
    "        model_id, \n",
    "        quantization_config = bnb_config, \n",
    "        torch_dtype = torch.float16,\n",
    "        device_map = {\"\":0}\n",
    ")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-09-27T01:54:13.970885Z",
     "iopub.status.idle": "2023-09-27T01:54:13.978029Z",
     "shell.execute_reply": "2023-09-27T01:54:13.977748Z",
     "shell.execute_reply.started": "2023-09-27T01:54:13.977715Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 1769472 || all params: 249347328 || trainable%: 0.7096414524241463\n"
     ]
    }
   ],
   "source": [
    "from peft import LoraConfig, get_peft_model, prepare_model_for_int8_training, TaskType\n",
    "\n",
    "# Define LoRA Config \n",
    "lora_config = LoraConfig(\n",
    " r=16,              # 4\n",
    " lora_alpha=32,     # 8\n",
    " target_modules=[\"q\", \"v\"],\n",
    " lora_dropout=0.05,\n",
    " bias=\"none\",\n",
    " task_type=TaskType.SEQ_2_SEQ_LM\n",
    ")\n",
    "\n",
    "# prepare int-8 model for training\n",
    "model = prepare_model_for_int8_training(model)\n",
    "\n",
    "# add LoRA adaptor\n",
    "model = get_peft_model(model, lora_config)\n",
    "model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a T5TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='4884' max='4884' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [4884/4884 34:14, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>0.964400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.718500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>0.778200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.715900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>0.791900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>0.776300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>350</td>\n",
       "      <td>0.703200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>0.738200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>450</td>\n",
       "      <td>0.763800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.626800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>550</td>\n",
       "      <td>0.684000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>0.729200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>650</td>\n",
       "      <td>0.677800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>0.688200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>750</td>\n",
       "      <td>0.763900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>0.683000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>850</td>\n",
       "      <td>0.710600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>900</td>\n",
       "      <td>0.654600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>950</td>\n",
       "      <td>0.668900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.661000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1050</td>\n",
       "      <td>0.721600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1100</td>\n",
       "      <td>0.714600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1150</td>\n",
       "      <td>0.662100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>0.715700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1250</td>\n",
       "      <td>0.706200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1300</td>\n",
       "      <td>0.639300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1350</td>\n",
       "      <td>0.733700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1400</td>\n",
       "      <td>0.630700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1450</td>\n",
       "      <td>0.669800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>0.659100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1550</td>\n",
       "      <td>0.637600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1600</td>\n",
       "      <td>0.700600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1650</td>\n",
       "      <td>0.642600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1700</td>\n",
       "      <td>0.689400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1750</td>\n",
       "      <td>0.661600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1800</td>\n",
       "      <td>0.549600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1850</td>\n",
       "      <td>0.589400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1900</td>\n",
       "      <td>0.567600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1950</td>\n",
       "      <td>0.720800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.561200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2050</td>\n",
       "      <td>0.606500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2100</td>\n",
       "      <td>0.645100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2150</td>\n",
       "      <td>0.671400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2200</td>\n",
       "      <td>0.560300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2250</td>\n",
       "      <td>0.584200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2300</td>\n",
       "      <td>0.550300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2350</td>\n",
       "      <td>0.637800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2400</td>\n",
       "      <td>0.618500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2450</td>\n",
       "      <td>0.654900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>0.612800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2550</td>\n",
       "      <td>0.574600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2600</td>\n",
       "      <td>0.591200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2650</td>\n",
       "      <td>0.582800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2700</td>\n",
       "      <td>0.613400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2750</td>\n",
       "      <td>0.608900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2800</td>\n",
       "      <td>0.633600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2850</td>\n",
       "      <td>0.650700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2900</td>\n",
       "      <td>0.610000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2950</td>\n",
       "      <td>0.604500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>0.572700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3050</td>\n",
       "      <td>0.557900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3100</td>\n",
       "      <td>0.582600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3150</td>\n",
       "      <td>0.543700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3200</td>\n",
       "      <td>0.661800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3250</td>\n",
       "      <td>0.684800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3300</td>\n",
       "      <td>0.561300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3350</td>\n",
       "      <td>0.530900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3400</td>\n",
       "      <td>0.532500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3450</td>\n",
       "      <td>0.589700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3500</td>\n",
       "      <td>0.509700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3550</td>\n",
       "      <td>0.563900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3600</td>\n",
       "      <td>0.547300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3650</td>\n",
       "      <td>0.479300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3700</td>\n",
       "      <td>0.583500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3750</td>\n",
       "      <td>0.576100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3800</td>\n",
       "      <td>0.541500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3850</td>\n",
       "      <td>0.499600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3900</td>\n",
       "      <td>0.574500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3950</td>\n",
       "      <td>0.614900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>0.580600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4050</td>\n",
       "      <td>0.506500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4100</td>\n",
       "      <td>0.541600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4150</td>\n",
       "      <td>0.573100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4200</td>\n",
       "      <td>0.504000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4250</td>\n",
       "      <td>0.519900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4300</td>\n",
       "      <td>0.535900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4350</td>\n",
       "      <td>0.516600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4400</td>\n",
       "      <td>0.572500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4450</td>\n",
       "      <td>0.550600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4500</td>\n",
       "      <td>0.503900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4550</td>\n",
       "      <td>0.575200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4600</td>\n",
       "      <td>0.583300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4650</td>\n",
       "      <td>0.516700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4700</td>\n",
       "      <td>0.543900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4750</td>\n",
       "      <td>0.475900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4800</td>\n",
       "      <td>0.485600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4850</td>\n",
       "      <td>0.482900</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=4884, training_loss=0.6191869320974889, metrics={'train_runtime': 2055.125, 'train_samples_per_second': 19.011, 'train_steps_per_second': 2.376, 'total_flos': 1.3061247910060032e+16, 'train_loss': 0.6191869320974889, 'epoch': 3.0})"
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import Seq2SeqTrainer, Seq2SeqTrainingArguments, DataCollatorForSeq2Seq\n",
    "\n",
    "training_args = Seq2SeqTrainingArguments(\n",
    "    output_dir = \"outputs\",\n",
    "    save_strategy = \"no\",\n",
    "    report_to = \"tensorboard\",\n",
    "    auto_find_batch_size = True,\n",
    "    warmup_steps = 100,\n",
    "    learning_rate = 1e-3, \n",
    "    weight_decay = 0.001, \n",
    "    fp16_full_eval = True, \n",
    "    fp16 = False,                         # 16 bits precision is sufficient and good\n",
    "    num_train_epochs = 3,\n",
    "    logging_strategy = \"steps\", \n",
    "    logging_steps = 100, \n",
    "#     max_steps = 2000,                   # disable if specifying no. of epochs\n",
    "#     gradient_accumulation_steps = 4,    # no. of updates steps to accumulate gradients, before updating it (higher = more accurate, but takes longer)\n",
    "#     optim='adamw_bnb_8bit', \n",
    "#     save_total_limit = 8,               # no. of checkpoints (models) saved in output_dir\n",
    "#     evaluation_strategy = 'epoch', \n",
    "#     logging_dir = f\"{output_dir}/logs\",\n",
    "\n",
    ")\n",
    "\n",
    "data_collator = DataCollatorForSeq2Seq(\n",
    "    tokenizer,\n",
    "    model = model,\n",
    "    label_pad_token_id = -100,   # we want to ignore tokenizer pad token in the loss\n",
    "    pad_to_multiple_of = 8\n",
    ")\n",
    "\n",
    "trainer = Seq2SeqTrainer(\n",
    "    model = model, \n",
    "    args = training_args, \n",
    "    data_collator = data_collator, \n",
    "    train_dataset = tokenized_dataset       # why when add the eval_dataset argument, training loss becomes 0\n",
    "    # if tokenized_dataset regenerated in 6b. (not loaded from disk), need to add in ['train'] indices\n",
    ")\n",
    "model.config.use_cache = False  # silence the warnings. Please re-enable for inference!\n",
    "\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 8. Saving model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total: 15.656 GB\n",
      "reserved: 6.164 GB\n",
      "allocated: 2.653 GB\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[9.2192768, 15.655829504]"
      ]
     },
     "execution_count": 164,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(f'total: {round(torch.cuda.get_device_properties(0).total_memory / 10**9, 3)} GB')\n",
    "print(f'reserved: {round(torch.cuda.memory_reserved(0) / 10**9, 3)} GB')     # reserved = allocated + cached\n",
    "print(f'allocated: {round(torch.cuda.memory_allocated(0) / 10**9, 3)} GB')\n",
    "\n",
    "[i/10**9 for i in torch.cuda.mem_get_info()]  # (free memory usage, total available memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('/home/dataiku/data/saved_models/flant5base_lora_squad/tokenizer_config.json',\n",
       " '/home/dataiku/data/saved_models/flant5base_lora_squad/special_tokens_map.json',\n",
       " '/home/dataiku/data/saved_models/flant5base_lora_squad/spiece.model',\n",
       " '/home/dataiku/data/saved_models/flant5base_lora_squad/added_tokens.json',\n",
       " '/home/dataiku/data/saved_models/flant5base_lora_squad/tokenizer.json')"
      ]
     },
     "execution_count": 165,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "peft_model_path = '<path>'\n",
    "trainer.model.save_pretrained(peft_model_path)\n",
    "tokenizer.save_pretrained(peft_model_path)   # not rly necessary unless changes made to tokenizer: add new tokens to its vocab, redefine special symbols such as '[CLS]', '[MASK]', '[SEP]', '[PAD]' etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "## To push model to HuggingFace\n",
    "\n",
    "# trainer.model.push_to_hub(\"<huggingface directory>\",\n",
    "#                   use_auth_token='<token>',\n",
    "#                   commit_message=\"v1\",\n",
    "#                   private=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 9. Model inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Overriding torch_dtype=None with `torch_dtype=torch.float16` due to requirements of `bitsandbytes` to enable model loading in mixed int8. Either pass torch_dtype=torch.float16 or don't pass this argument at all to remove this warning.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Peft model loaded\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from peft import PeftModel, PeftConfig\n",
    "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer\n",
    "\n",
    "# Load peft config for pre-trained checkpoint etc. \n",
    "peft_model_path = '<saved model path>'\n",
    "config = PeftConfig.from_pretrained(peft_model_path)\n",
    "\n",
    "# load base LLM model and tokenizer\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(config.base_model_name_or_path, \n",
    "                                              return_dict=True, \n",
    "                                              load_in_8bit=True,    # True if quantizing\n",
    "                                              device_map={\"\":0})\n",
    "tokenizer = AutoTokenizer.from_pretrained(config.base_model_name_or_path)\n",
    "\n",
    "# Load the Lora model\n",
    "peft_model = PeftModel.from_pretrained(model, peft_model_path, device_map={\"\":0})\n",
    "peft_model.eval()\n",
    "\n",
    "print(\"Peft model loaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer\n",
    "\n",
    "model_id = \"google/flan-t5-base\"\n",
    "\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(\n",
    "        model_id, \n",
    "        quantization_config = bnb_config, \n",
    "        torch_dtype = torch.float16,\n",
    "        device_map = {\"\":0}\n",
    ")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-09-27T01:54:13.996639Z",
     "iopub.status.idle": "2023-09-27T01:54:13.997269Z",
     "shell.execute_reply": "2023-09-27T01:54:13.997096Z",
     "shell.execute_reply.started": "2023-09-27T01:54:13.997076Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model:\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "Cheese"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "peft_model:\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "Cheese"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "context = \"Cheese is the best food.\"\n",
    "question = \"What is the best food?\"\n",
    "\n",
    "print('model:')\n",
    "make_inference(model, context, question)\n",
    "print('peft_model:')\n",
    "make_inference(peft_model, context, question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-09-27T01:54:13.998399Z",
     "iopub.status.idle": "2023-09-27T01:54:13.999126Z",
     "shell.execute_reply": "2023-09-27T01:54:13.998934Z",
     "shell.execute_reply.started": "2023-09-27T01:54:13.998914Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model:\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "The Moon is approximately 1.3 billion light years away."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "peft_model:\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "a distance of 0.002 miles"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "context = \"Cheese is the best food.\"\n",
    "question = \"How far away is the Moon from the Earth?\"\n",
    "\n",
    "print('model:')\n",
    "make_inference(model, context, question)\n",
    "print('peft_model:')\n",
    "make_inference(peft_model, context, question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model:\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "30 times Earth's diameter"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "peft_model:\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "384,400 km (238,900 miles),"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "context = \"The Moon orbits Earth at an average distance of 384,400 km (238,900 miles), or about 30 times Earth's diameter. Its gravitational influence is the main driver of Earth's tides and very slowly lengthens Earth's day. The Moon's orbit around Earth has a sidereal period of 27.3 days. During each synodic period of 29.5 days, the amount of visible surface illuminated by the Sun varies from none up to 100%, resulting in lunar phases that form the basis for the months of a lunar calendar. The Moon is tidally locked to Earth, which means that the length of a full rotation of the Moon on its own axis causes its same side (the near side) to always face Earth, and the somewhat longer lunar day is the same as the synodic period. However, 59% of the total lunar surface can be seen from Earth through cyclical shifts in perspective known as libration.\"\n",
    "question = \"At what distance does the Moon orbit the Earth?\"\n",
    "\n",
    "print('model:')\n",
    "make_inference(model, context, question)\n",
    "print('peft_model:')\n",
    "make_inference(peft_model, context, question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model:\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "Because there is no ability to experimentally control the nature of the damage, however, this information is often difficult to interpret"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "peft_model:\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "there is no ability to experimentally control the nature"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "## Basic\n",
    "\n",
    "context = f\"\"\"\n",
    "Another approach to brain function is to examine the consequences of damage to specific brain areas. \n",
    "Even though it is protected by the skull and meninges, surrounded by cerebrospinal fluid, \n",
    "and isolated from the bloodstream by the blood–brain barrier, \n",
    "the delicate nature of the brain makes it vulnerable to numerous diseases and several types of damage. \n",
    "In humans, the effects of strokes and other types of brain damage have been a key source of information about brain function. \n",
    "Because there is no ability to experimentally control the nature of the damage, however, \n",
    "this information is often difficult to interpret. In animal studies, most commonly involving rats, \n",
    "it is possible to use electrodes or loclly injected chemicals to produce precise patterns of damage \n",
    "and then examine the consequences for behavior.\n",
    "\"\"\"\n",
    "question = \"Why is it difficult to study the brain?\"\n",
    "\n",
    "print('model:')\n",
    "make_inference(model, context, question)\n",
    "print('peft_model:')\n",
    "make_inference(peft_model, context, question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model:\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "In animal studies, most commonly involving rats, it is possible to use electrodes or locally injected chemicals to produce precise patterns of damage and then examine the consequences for behavior."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "peft_model:\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "use electrodes or locally injected chemicals to produce"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "## Intermediate\n",
    "\n",
    "context = f\"\"\"\n",
    "Another approach to brain function is to examine the consequences of damage to specific brain areas. \n",
    "Even though it is protected by the skull and meninges, surrounded by cerebrospinal fluid, \n",
    "and isolated from the bloodstream by the blood–brain barrier, \n",
    "the delicate nature of the brain makes it vulnerable to numerous diseases and several types of damage. \n",
    "In humans, the effects of strokes and other types of brain damage have been a key source of information about brain function. \n",
    "Because there is no ability to experimentally control the nature of the damage, however, \n",
    "this information is often difficult to interpret. In animal studies, most commonly involving rats, \n",
    "it is possible to use electrodes or locally injected chemicals to produce precise patterns of damage \n",
    "and then examine the consequences for behavior.\n",
    "\"\"\"\n",
    "question = \"How do we check for brain damage?\"\n",
    "\n",
    "print('model:')\n",
    "make_inference(model, context, question)\n",
    "print('peft_model:')\n",
    "make_inference(peft_model, context, question)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 10. Model evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Helper functions\n",
    "\n",
    "def normalize_text(s):\n",
    "    \"\"\"Removing articles and punctuation, and standardizing whitespace are all typical text processing steps.\"\"\"\n",
    "    import string, re\n",
    "\n",
    "    def remove_articles(text):\n",
    "        regex = re.compile(r\"\\b(a|an|the)\\b\", re.UNICODE)\n",
    "        return re.sub(regex, \" \", text)\n",
    "\n",
    "    def white_space_fix(text):\n",
    "        return \" \".join(text.split())\n",
    "\n",
    "    def remove_punc(text):\n",
    "        exclude = set(string.punctuation)\n",
    "        return \"\".join(ch for ch in text if ch not in exclude)\n",
    "\n",
    "    def lower(text):\n",
    "        return text.lower()\n",
    "\n",
    "    return white_space_fix(remove_articles(remove_punc(lower(s))))\n",
    "\n",
    "def compute_exact_match(prediction, truth):\n",
    "    return int(normalize_text(prediction) == normalize_text(truth))\n",
    "\n",
    "def compute_f1(prediction, truth):\n",
    "    pred_tokens = normalize_text(prediction).split()\n",
    "    truth_tokens = normalize_text(truth).split()\n",
    "    \n",
    "    # if either the prediction or the truth is no-answer then f1 = 1 if they agree, 0 otherwise\n",
    "    if len(pred_tokens) == 0 or len(truth_tokens) == 0:\n",
    "        return int(pred_tokens == truth_tokens)\n",
    "    \n",
    "    common_tokens = set(pred_tokens) & set(truth_tokens)\n",
    "    \n",
    "    # if there are no common tokens then f1 = 0\n",
    "    if len(common_tokens) == 0:\n",
    "        return 0\n",
    "    \n",
    "    prec = len(common_tokens) / len(pred_tokens)\n",
    "    rec = len(common_tokens) / len(truth_tokens)\n",
    "    \n",
    "    return 2 * (prec * rec) / (prec + rec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1737/1737 [08:48<00:00,  3.29it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6424420525932688\n",
      "0.514104778353483\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_from_disk\n",
    "from tqdm import tqdm\n",
    "\n",
    "## function to generate predictions\n",
    "def evaluate_peft_model(sample, max_target_length=200):\n",
    "    outputs = model.generate(input_ids=sample[\"input_ids\"].unsqueeze(0).cuda(), do_sample=True, top_p=0.9, max_new_tokens=max_target_length)    \n",
    "    prediction = tokenizer.decode(outputs[0].detach().cpu().numpy(), skip_special_tokens=True)\n",
    "    labels = np.where(sample['labels'] != -100, sample['labels'], tokenizer.pad_token_id)    # Replace -100 in the labels as cannot be decoded\n",
    "    labels = tokenizer.decode(labels, skip_special_tokens=True)\n",
    "\n",
    "    # Some simple post-processing\n",
    "    return prediction, labels\n",
    "\n",
    "## load test dataset from distk\n",
    "test_dataset = load_from_disk('/home/dataiku/data/saved_datasets/squad/tokenized/test').with_format(\"torch\")\n",
    "\n",
    "## compute score\n",
    "f1_scores, exact_scores = [], []\n",
    "for sample in tqdm(test_dataset, miniters=100, maxinterval=float(\"inf\"), position=0, leave=True):\n",
    "    p, l = evaluate_peft_model(sample)\n",
    "    f1_scores.append(compute_f1(p, l))\n",
    "    exact_scores.append(compute_exact_match(p, l))\n",
    "\n",
    "print(np.mean(f1_scores))\n",
    "print(np.mean(exact_scores))"
   ]
  }
 ],
 "metadata": {
  "createdOn": 1695790872726,
  "creator": "jovi",
  "customFields": {},
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python (env test_GPU)",
   "language": "python",
   "name": "py-dku-venv-test_gpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  },
  "modifiedBy": "jovi",
  "tags": [],
  "versionNumber": 1
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
